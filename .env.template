### Template .env
# Set to true to use local Ollama integration instead of Gemini
USE_OLLAMA=true

# Ollama HTTP host (optional). If set, the script will try HTTP first.
# Example: http://localhost:11434
OLLAMA_HOST=http://localhost:11434

# Ollama model name to use (CLI or HTTP)
OLLAMA_MODEL=qwen3-vl:4b

# Path to ollama CLI binary if it's not in PATH
OLLAMA_CLI_PATH=ollama

# If Ollama fails, optionally fall back to Gemini
OLLAMA_FALLBACK_TO_GEMINI=false

# Google API key for Gemini (used only if USE_OLLAMA is not true or fallback)
GOOGLE_API_KEY=AIzaSyC5fl-od70m11MAHEYOI0PrDLIV2Kyyux4
